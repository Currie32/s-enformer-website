{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Abstract It is well established that regulatory sequences in DNA can alter gene expression. It is also known that variants in these regulatory sequences can lead to changes in the level of gene expression. On top of this, variant effects are cell type specific despite DNA being nearly identical in all cells. Understanding this relationship is critical as adverse changes in gene expression can lead to diseases and syndromes such as cancer, autoimmune, neurological disorders, and many others. However, there is not yet a model to accurately map variations in regulatory sequences to changes in gene expression across all human cell and tissue types. Complexities in modelling arise from regulatory elements being located a significant distance from the gene being expressed, non-linear interactions between genomic regions, and regulatory variations in different cell and tissue types. Ab initio deep learning models have made significant advancements on these two fronts in recent years, yet current methods are still limited in their ability to include distal genomic regions due to their model architecture. The current state-of-the-art method, Enformer, uses transformers with a self-attention mechanism that causes a quadratic increase in memory as the sequence length expands. To help maximise the amount of DNA that can be included in the model, we propose replacing self-attention with sparse-attention from the BigBird model. This attention mechanism results in the memory usage increasing linearly with sequence length. This allows for significantly longer DNA sequences to be used in modelling, at the same memory capacity, which should result in more accurate gene expression predictions. Lay Summary Accurate predictions of gene expression can lead to better health outcomes. These predictions can help diagnose a disease and understand its severity, which can assist medical professionals in selecting the most suitable treatment. The challenge of predicting gene expression lies in the number and location of regulatory elements in DNA. These regulatory elements can be located more than 100,000 bases away from the gene being expressed and multiple elements can simultaneously affect the expression of a gene, leading to complex calculations. Further complications arise from gene regulation differing across cell and tissue types. Nonetheless, with advances in deep learning models and increasingly large datasets, the accuracy of predictions have significantly improved. The trouble is that current methods, such as the state-of-the-art model Enformer, is limited in how much DNA it can use to predict gene expression. Enformer\u2019s memory usage increases significantly with the length of the input DNA sequence due to its architecture. We propose a new model that replaces Enformer\u2019s computationally intensive layers (the attention layers) with those from the BigBird model, which only scale linearly with the length of the input DNA sequence. Using these attention layers allows a longer sequence of DNA to be used by the model, which should lead to more accurate predictions of gene expression. Glossary Attention: A mechanism used by Transformers that provides more important inputs with greater weight when making predictions. Enformer: Current state-of-the-art model for predicting gene expression from a sequence of DNA. Genomic track: A sequence of DNA described by a set of properties. Multi-task learning: Training a machine learning model on multiple related task at once. Model architecture: The layers in a deep learning model that transform the input into the expected output. Receptive field: The subset of a model\u2019s input that impacts the prediction. Transformer: A deep learning model consisting of an encoder and/or decoder, which uses attention to process an input sequence in parallel. Abbreviations ATAC-Seq: Assay of transposase accessible chromatin sequencing bp: Base pairs CAGE: Cap analysis of gene expression ChIP: Chromatin immunoprecipitation CNN: Convolutional neural network DNase-Seq: Deoxyribonuclease sequencing GELU: Gaussian error linear unit GWAS: Genome wide association studies TF: Transcription factor TPU: Tensor processing unit Word Count This report contains 4,785 words.","title":"Abstract & Lay Summary"},{"location":"#abstract","text":"It is well established that regulatory sequences in DNA can alter gene expression. It is also known that variants in these regulatory sequences can lead to changes in the level of gene expression. On top of this, variant effects are cell type specific despite DNA being nearly identical in all cells. Understanding this relationship is critical as adverse changes in gene expression can lead to diseases and syndromes such as cancer, autoimmune, neurological disorders, and many others. However, there is not yet a model to accurately map variations in regulatory sequences to changes in gene expression across all human cell and tissue types. Complexities in modelling arise from regulatory elements being located a significant distance from the gene being expressed, non-linear interactions between genomic regions, and regulatory variations in different cell and tissue types. Ab initio deep learning models have made significant advancements on these two fronts in recent years, yet current methods are still limited in their ability to include distal genomic regions due to their model architecture. The current state-of-the-art method, Enformer, uses transformers with a self-attention mechanism that causes a quadratic increase in memory as the sequence length expands. To help maximise the amount of DNA that can be included in the model, we propose replacing self-attention with sparse-attention from the BigBird model. This attention mechanism results in the memory usage increasing linearly with sequence length. This allows for significantly longer DNA sequences to be used in modelling, at the same memory capacity, which should result in more accurate gene expression predictions.","title":"Abstract"},{"location":"#lay-summary","text":"Accurate predictions of gene expression can lead to better health outcomes. These predictions can help diagnose a disease and understand its severity, which can assist medical professionals in selecting the most suitable treatment. The challenge of predicting gene expression lies in the number and location of regulatory elements in DNA. These regulatory elements can be located more than 100,000 bases away from the gene being expressed and multiple elements can simultaneously affect the expression of a gene, leading to complex calculations. Further complications arise from gene regulation differing across cell and tissue types. Nonetheless, with advances in deep learning models and increasingly large datasets, the accuracy of predictions have significantly improved. The trouble is that current methods, such as the state-of-the-art model Enformer, is limited in how much DNA it can use to predict gene expression. Enformer\u2019s memory usage increases significantly with the length of the input DNA sequence due to its architecture. We propose a new model that replaces Enformer\u2019s computationally intensive layers (the attention layers) with those from the BigBird model, which only scale linearly with the length of the input DNA sequence. Using these attention layers allows a longer sequence of DNA to be used by the model, which should lead to more accurate predictions of gene expression.","title":"Lay Summary"},{"location":"#glossary","text":"Attention: A mechanism used by Transformers that provides more important inputs with greater weight when making predictions. Enformer: Current state-of-the-art model for predicting gene expression from a sequence of DNA. Genomic track: A sequence of DNA described by a set of properties. Multi-task learning: Training a machine learning model on multiple related task at once. Model architecture: The layers in a deep learning model that transform the input into the expected output. Receptive field: The subset of a model\u2019s input that impacts the prediction. Transformer: A deep learning model consisting of an encoder and/or decoder, which uses attention to process an input sequence in parallel.","title":"Glossary"},{"location":"#abbreviations","text":"ATAC-Seq: Assay of transposase accessible chromatin sequencing bp: Base pairs CAGE: Cap analysis of gene expression ChIP: Chromatin immunoprecipitation CNN: Convolutional neural network DNase-Seq: Deoxyribonuclease sequencing GELU: Gaussian error linear unit GWAS: Genome wide association studies TF: Transcription factor TPU: Tensor processing unit","title":"Abbreviations"},{"location":"#word-count","text":"This report contains 4,785 words.","title":"Word Count"},{"location":"background/","text":"Genome wide association studies (GWAS) have identified thousands of potential genetic variants that are associated with human diseases and traits 1 . The vast majority of these variants are located in non-coding regions, suggesting that gene regulation has a significant role in the health of an individual 2 , 3 . Adverse changes to gene expression have been linked to an increased risk of disease 2 , 4 . It is therefore critical to identify the genes regulated by disease-associated non-coding variants to help develop therapeutic targets. However, due to the limitations of GWAS, these studies are incapable of regularly finding the causal variant. They are limited to analysing only common variants and linkage disequilibrium makes it challenging to know if a variant is causal or not 5 . Even if a variant was determined to be casual, its role would still be unclear, e.g. what gene it regulates. The shortcomings of GWAS can be alleviated by advances in deep learning models 6 , 7 , 8 , 9 , 10 , 5 . These models predict gene expression, using only a sequence of DNA as input, to help understand the impact of non-coding variants. The early work of DeepSEA demonstrated the feasibility of using deep convolutional neural networks (CNN) to predict gene expression from a sequence of DNA 6 . This outperformed the previous state-of-the-art method, a support vector machine, as the authors showed that deep CNNs can better aggregate the non-linear interactions across a DNA sequence that result in the gene expression level 11 . Further advancements in deep learning models were made by Basset 7 , Basenji 8 , ExPecto 9 , DeepFun 10 , and Enformer 5 . These models used data from an increasing number of cell and tissue types, a longer input sequence of DNA, and more advanced model architectures to achieve better results. It is necessary for a model to be trained using a variety of cell and tissue types, as transcription regulation can differ between them 12 . In addition, disease-causing variants tend to alter gene regulation within specific cell and tissues types 13 . Therefore, it is critical that a model can generalise to an extensive set of cell and tissue types to maximise its utility. Fortunately, many organisations have annotated epigenetic and expression profiles in a wide range of cell types 14 , 15 , 16 . Recent deep learning approaches have leveraged these resources to create training, validation, and testing datasets. Some approaches have even included non-human data, such as from mice 17 . The author demonstrated that a model trained on both human and mouse data outperformed a model trained solely on human data, thus demonstrating that the conserved nature of non-coding DNA between organisms can be used to improve a model\u2019s accuracy 18 , 19 . An additional benefit is that non-human samples can be from cells and tissues that are unable to be easily collected from humans (e.g. brain tissue), thus serving as very useful training examples 17 . The length of the input DNA sequence used by deep learning models has increased significantly from 1,000 base pairs (bp) for DeepSEA 6 to almost 200,000 bp in Enformer 5 . Increasing the amount of DNA used by a model is essential as some regulatory elements, such as enhancers, influence the expression of a gene from hundreds of thousands of base pairs away 20 , 21 . Some research estimates that 16% of enhancer-gene pairs could be more than 100,000 bp away 5 . Enhancers are able to influence gene expression from long distances due to the non-linear shape of DNA. Loops are formed in DNA, which bring enhancers close to their target genes, allowing the interactions to take place 22 . For models to achieve even better results, they will require longer input DNA sequences to capture the remaining regulatory elements that are currently out of range. The architecture of deep learning models has evolved in recent years. Advances have been made by using more convolutional layers to learn more abstract features 9 , densely connected dilated convolutions to share information across longer distances 8 , and transformers with self-attention to increase the receptive field of the model and more accurately capture distal information 5 . At a high level , a Transformer model (or transformer layer) is composed of an encoder and/or a decoder unit, which allows it to process an input sequence in parallel. It is the successor of recurrent neural networks, which are limited to processing an input sequence sequentially, thereby taking longer to train. Transformers also benefit from an attention mechanism, which helps them to learn longer sequences by taking a weighted average of the input tokens. The average is weighted by the significance of each word with respect to the output, so the most relevant tokens receive the most weight (i.e. the most attention). An issue arises when the input sequence becomes too long. For models that use self-attention, each token in a sequence attends to all others, yielding n 2 weighted averages computed. This results in the memory usage of a model scaling quadratically with sequence length. Enformer, the current state-of-the-art model, suffers from this limitation. To overcome this constraint, we propose a new model, Sparse-Enformer (S-Enformer) that replaces the self-attention with sparse-attention from the BigBird model 23 . Using sparse-attention reduces the quadratic increase in memory usage to linear. This allows for a significantly longer DNA sequence to be used as input to a model, which should result in further accuracy improvements. We also demonstrate that S-Enformer can be trained and evaluated in exactly the same method as Enformer, which simplifies the comparison process. Uffelmann, Emil, Qin Qin Huang, Nchangwi Syntia Munung, Jantina de Vries, Yukinori Okada, Alicia R. Martin, Hilary C. Martin, Tuuli Lappalainen, and Danielle Posthuma. 2021. \u2018Genome-Wide Association Studies\u2019. Nature Reviews Methods Primers 1 (1): 1\u201321. https://doi.org/10.1038/s43586-021-00056-9 . \u21a9 Edwards, Stacey L., Jonathan Beesley, Juliet D. French, and Alison M. Dunning. 2013. \u2018Beyond GWASs: Illuminating the Dark Road from Association to Function\u2019. The American Journal of Human Genetics 93 (5): 779\u201397. https://doi.org/10.1016/j.ajhg.2013.10.012 . \u21a9 \u21a9 Leslie, R., C. J. O\u2019Donnell, and A. D. Johnson. 2014. \u2018GRASP: Analysis of Genotype-Phenotype Results from 1390 Genome-Wide Association Studies and Corresponding Open Access Database\u2019. Bioinformatics 30 (12): i185\u201394. https://doi.org/10.1093/bioinformatics/btu273 . \u21a9 Albert, Frank W., and Leonid Kruglyak. 2015. \u2018The Role of Regulatory Variation in Complex Traits and Disease\u2019. Nature Reviews Genetics 16 (4): 197\u2013212. https://doi.org/10.1038/nrg3891 . \u21a9 Avsec, \u017diga, Vikram Agarwal, Daniel Visentin, Joseph R. Ledsam, Agnieszka Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R. Kelley. 2021. \u2018Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions\u2019. Nature Methods 18 (10): 1196\u20131203. https://doi.org/10.1038/s41592-021-01252-x . \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 Zhou, Jian, and Olga G. Troyanskaya. 2015. \u2018Predicting Effects of Noncoding Variants with Deep Learning\u2013Based Sequence Model\u2019. Nature Methods 12 (10): 931\u201334. https://doi.org/10.1038/nmeth.3547 . \u21a9 \u21a9 \u21a9 Kelley, David R., Jasper Snoek, and John L. Rinn. 2016. \u2018Basset: Learning the Regulatory Code of the Accessible Genome with Deep Convolutional Neural Networks\u2019. Genome Research 26 (7): 990\u201399. https://doi.org/10.1101/gr.200535.115 . \u21a9 \u21a9 Kelley, David R., Yakir A. Reshef, Maxwell Bileschi, David Belanger, Cory Y. McLean, and Jasper Snoek. 2018. \u2018Sequential Regulatory Activity Prediction across Chromosomes with Convolutional Neural Networks\u2019. Genome Research 28 (5): 739\u201350. https://doi.org/10.1101/gr.227819.117 . \u21a9 \u21a9 \u21a9 Zhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K. Wong, and Olga G. Troyanskaya. 2018. \u2018Deep Learning Sequence-Based Ab Initio Prediction of Variant Effects on Expression and Disease Risk\u2019. Nature Genetics 50 (8): 1171\u201379. https://doi.org/10.1038/s41588-018-0160-6 . \u21a9 \u21a9 \u21a9 Pei, Guangsheng, Ruifeng Hu, Yulin Dai, Astrid Marilyn Manuel, Zhongming Zhao, and Peilin Jia. 2021. \u2018Predicting Regulatory Variants Using a Dense Epigenomic Mapped CNN Model Elucidated the Molecular Basis of Trait-Tissue Associations\u2019. Nucleic Acids Research 49 (1): 53\u201366. https://doi.org/10.1093/nar/gkaa1137 . \u21a9 \u21a9 Mamoshina, Polina, Armando Vieira, Evgeny Putin, and Alex Zhavoronkov. 2016. \u2018Applications of Deep Learning in Biomedicine\u2019. Molecular Pharmaceutics 13 (5): 1445\u201354. https://doi.org/10.1021/acs.molpharmaceut.5b00982 . \u21a9 Hobert, Oliver. 2008. \u2018Gene Regulation by Transcription Factors and MicroRNAs\u2019. Science 319 (5871): 1785\u201386. https://doi.org/10.1126/science.1151651 . \u21a9 Aguet, Fran\u00e7ois, Andrew A. Brown, Stephane E. Castel, Joe R. Davis, Yuan He, Brian Jo, Pejman Mohammadi, et al. 2017. \u2018Genetic Effects on Gene Expression across Human Tissues\u2019. Nature 550 (7675): 204\u201313. https://doi.org/10.1038/nature24277 . \u21a9 The ENCODE Project Consortium. 2012. \u2018An Integrated Encyclopedia of DNA Elements in the Human Genome\u2019. Nature 489 (7414): 57. https://doi.org/10.1038/nature11247 . \u21a9 Forrest et al. 2014. \u2018A Promoter-Level Mammalian Expression Atlas\u2019. Nature 507 (7493): 462. https://doi.org/10.1038/nature13182 . \u21a9 Roadmap Epigenomics Consortium, Wouter Kundaje, Jason Ernst, Misha Bilenky, Angela Yen, Alireza Heravi-Moussavi, Pouya Kheradpour, et al. 2015. \u2018Integrative Analysis of 111 Reference Human Epigenomes\u2019. Nature 518 (7539): 317\u201330. https://doi.org/10.1038/nature14248 . \u21a9 Kelley, David R. 2020. \u2018Cross-Species Regulatory Sequence Activity Prediction\u2019. PLOS Computational Biology 16 (7): e1008050. https://doi.org/10.1371/journal.pcbi.1008050 . \u21a9 \u21a9 Woolfe, Adam, Martin Goodson, Debbie K. Goode, Phil Snell, Gayle K. McEwen, Tanya Vavouri, Sarah F. Smith, et al. 2004. \u2018Highly Conserved Non-Coding Sequences Are Associated with Vertebrate Development\u2019. PLOS Biology 3 (1): e7. https://doi.org/10.1371/journal.pbio.0030007 . \u21a9 Pennacchio, Len A., Nadav Ahituv, Alan M. Moses, Shyam Prabhakar, Marcelo A. Nobrega, Malak Shoukry, Simon Minovitsky, et al. 2006. \u2018In Vivo Enhancer Analysis of Human Conserved Non-Coding Sequences\u2019. Nature 444 (7118): 499\u2013502. https://doi.org/10.1038/nature05295 . \u21a9 Levine, Mike. 2010. \u2018Transcriptional Enhancers in Animal Development and Evolution\u2019. Current Biology 20 (17): R754\u201363. https://doi.org/10.1016/j.cub.2010.06.070 . \u21a9 Long, Hannah K., Sara L. Prescott, and Joanna Wysocka. 2016. \u2018Ever-Changing Landscapes: Transcriptional Enhancers in Development and Evolution\u2019. Cell 167 (5): 1170\u201387. https://doi.org/10.1016/j.cell.2016.09.018 . \u21a9 Krivega, Ivan, and Ann Dean. 2012. \u2018Enhancer and Promoter Interactions \u2014 Long Distance Calls\u2019. Current Opinion in Genetics & Development 22 (2): 79. https://doi.org/10.1016/j.gde.2011.11.001 . \u21a9 Zaheer, Manzil, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, et al. 2021. \u2018Big Bird: Transformers for Longer Sequences\u2019. ArXiv:2007.14062 [Cs, Stat], January. http://arxiv.org/abs/2007.14062 . \u21a9","title":"Background"},{"location":"conclusion/","text":"Conclusion This work has demonstrated the possible benefits of using sparse-attention to predict gene expression levels from a sequence of DNA. Specifically, longer DNA sequences can be used as input to the model, which should achieve more accurate gene expression predictions, while using relatively less memory than if the model used self-attention. Although conclusive results could not be achieved due to time and computational constraints, it is promising that advancements can be made on the current state-of-the-art. Future Work There are a number of promising directions in which to take this work. The first is to confirm if accuracy parity can be achieved when all of the weights in the model are frozen during training except for those in the attention layers. This would be a valuable finding as it would save a significant amount of time and resources compared to training a model from scratch. However, it is uncertain if this is possible given our training results. Assuming this first step fails, the next step would be to confirm that BigBird\u2019s sparse-attention can achieve performance parity with Enformer by training the model from scratch. Given that BigBird\u2019s sparse-attention mechanism is meant to match the performance of self-attention, it is expected that parity will be achieved 1 . After achieving accuracy parity, the aim is to improve upon these results. Using the maximum possible sequence length as input to the model and training it from scratch should help to improve its gene expression predictions. It would be worthwhile at this point to compare the model with one using a different sparse-attention mechanism. The Linear Transformer 2 would be an excellent choice as it was the top scoring model in the text classification task from the Long Range Arena benchmark 3 . Since this model used less memory than BigBird in the benchmark study, it is possible that it can use a longer DNA sequence as input and achieve even better results. Based on the BigBird and Linear Transformer performance comparison, the better scoring model could be used to improve upon nucleotide-resolution predictions. For example, if the model, after being retrained on the single-cell data, can outperform BPNet 4 at predicting binding profiles of TFs with only slight changes to its architecture, to account for the differently sized input and output, it would demonstrate its generalisability. The model\u2019s architecture could then be slightly altered to allow it to perform multi-task learning . Given the promise of state-of-the-art results on the 128 bp and nucleotide-resolution tasks, it could score even better results by learning both of these tasks at once with their combined training data. Acknowledgements We thank Alan Murphy and Dr. Nathan Skene for their assistance and guidance throughout this project. Code Availability All the code to reproduce this work is available at: https://github.com/Currie32/s-enformer Data Availability All the training, validation, and testing data is available at: https://console.cloud.google.com/storage/browser/basenji_barnyard/data Ethics Declaration There are no ethical conflicts for this work. Although human data is used, it is publicly available and non-identifiable. Zaheer, Manzil, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, et al. 2021. \u2018Big Bird: Transformers for Longer Sequences\u2019. ArXiv:2007.14062 [Cs, Stat], January. http://arxiv.org/abs/2007.14062 . \u21a9 Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. 2020. \u2018Transformers Are RNNs: Fast Autoregressive Transformers with Linear Attention\u2019. ArXiv:2006.16236 [Cs, Stat], August. http://arxiv.org/abs/2006.16236 . \u21a9 Tay, Yi, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2020. \u2018Long Range Arena: A Benchmark for Efficient Transformers\u2019. ArXiv:2011.04006 [Cs], November. http://arxiv.org/abs/2011.04006 . \u21a9 Avsec, \u017diga, Melanie Weilert, Avanti Shrikumar, Sabrina Krueger, Amr Alexandari, Khyati Dalal, Robin Fropf, et al. 2021. \u2018Base-Resolution Models of Transcription-Factor Binding Reveal Soft Motif Syntax\u2019. Nature Genetics 53 (3): 354\u201366. https://doi.org/10.1038/s41588-021-00782-6 . \u21a9","title":"Conclusion & Future Work"},{"location":"conclusion/#conclusion","text":"This work has demonstrated the possible benefits of using sparse-attention to predict gene expression levels from a sequence of DNA. Specifically, longer DNA sequences can be used as input to the model, which should achieve more accurate gene expression predictions, while using relatively less memory than if the model used self-attention. Although conclusive results could not be achieved due to time and computational constraints, it is promising that advancements can be made on the current state-of-the-art.","title":"Conclusion"},{"location":"conclusion/#future-work","text":"There are a number of promising directions in which to take this work. The first is to confirm if accuracy parity can be achieved when all of the weights in the model are frozen during training except for those in the attention layers. This would be a valuable finding as it would save a significant amount of time and resources compared to training a model from scratch. However, it is uncertain if this is possible given our training results. Assuming this first step fails, the next step would be to confirm that BigBird\u2019s sparse-attention can achieve performance parity with Enformer by training the model from scratch. Given that BigBird\u2019s sparse-attention mechanism is meant to match the performance of self-attention, it is expected that parity will be achieved 1 . After achieving accuracy parity, the aim is to improve upon these results. Using the maximum possible sequence length as input to the model and training it from scratch should help to improve its gene expression predictions. It would be worthwhile at this point to compare the model with one using a different sparse-attention mechanism. The Linear Transformer 2 would be an excellent choice as it was the top scoring model in the text classification task from the Long Range Arena benchmark 3 . Since this model used less memory than BigBird in the benchmark study, it is possible that it can use a longer DNA sequence as input and achieve even better results. Based on the BigBird and Linear Transformer performance comparison, the better scoring model could be used to improve upon nucleotide-resolution predictions. For example, if the model, after being retrained on the single-cell data, can outperform BPNet 4 at predicting binding profiles of TFs with only slight changes to its architecture, to account for the differently sized input and output, it would demonstrate its generalisability. The model\u2019s architecture could then be slightly altered to allow it to perform multi-task learning . Given the promise of state-of-the-art results on the 128 bp and nucleotide-resolution tasks, it could score even better results by learning both of these tasks at once with their combined training data.","title":"Future Work"},{"location":"conclusion/#acknowledgements","text":"We thank Alan Murphy and Dr. Nathan Skene for their assistance and guidance throughout this project.","title":"Acknowledgements"},{"location":"conclusion/#code-availability","text":"All the code to reproduce this work is available at: https://github.com/Currie32/s-enformer","title":"Code Availability"},{"location":"conclusion/#data-availability","text":"All the training, validation, and testing data is available at: https://console.cloud.google.com/storage/browser/basenji_barnyard/data","title":"Data Availability"},{"location":"conclusion/#ethics-declaration","text":"There are no ethical conflicts for this work. Although human data is used, it is publicly available and non-identifiable. Zaheer, Manzil, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, et al. 2021. \u2018Big Bird: Transformers for Longer Sequences\u2019. ArXiv:2007.14062 [Cs, Stat], January. http://arxiv.org/abs/2007.14062 . \u21a9 Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. 2020. \u2018Transformers Are RNNs: Fast Autoregressive Transformers with Linear Attention\u2019. ArXiv:2006.16236 [Cs, Stat], August. http://arxiv.org/abs/2006.16236 . \u21a9 Tay, Yi, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2020. \u2018Long Range Arena: A Benchmark for Efficient Transformers\u2019. ArXiv:2011.04006 [Cs], November. http://arxiv.org/abs/2011.04006 . \u21a9 Avsec, \u017diga, Melanie Weilert, Avanti Shrikumar, Sabrina Krueger, Amr Alexandari, Khyati Dalal, Robin Fropf, et al. 2021. \u2018Base-Resolution Models of Transcription-Factor Binding Reveal Soft Motif Syntax\u2019. Nature Genetics 53 (3): 354\u201366. https://doi.org/10.1038/s41588-021-00782-6 . \u21a9","title":"Ethics Declaration"},{"location":"discussion/","text":"Sparse-attention should lead to more accurate predictions Deep learning models have made significant gains in predicting gene expression from a sequence of DNA. A key limitation of these models is that they still lack the ability to use long enough sequences to include all regulatory elements in their predictions. By replacing the self-attention mechanism from the Enformer model with BigBird\u2019s sparse-attention, we have taken one step closer to overcoming this obstacle. Although we lacked the time and computational resources to train the model from scratch and demonstrate that using sparse-attention and a longer input sequence will achieve more accurate results, we are confident that this will be the case. This confidence is based on the state-of-the-art results achieved by BigBird 1 , 2 from its novel architecture and earlier models scoring greater accuracy through the use of longer input sequences 3 , 4 , 5 . Limitations to model training A possible explanation for not achieving accuracy parity with Enformer is the constraint placed on the model during training. When a model is being trained from scratch, all of its weights update in order to reduce the loss. This allows the model to make the best use of its architecture. When training S-Enformer, the model could not update in a cohesive way as all of the weights were frozen except for those in the sparse-attention layers. The sparse-attention layers\u2019 weights were forced to update in a constrained environment and possibly not make optimal use of their design. Given that the self-attention layers have a different architecture to the sparse-attention layers, it might not be possible for the model to achieve its optimal performance when only the sparse-attention layer weights can update. Parameter optimisation could improve results Optimising S-Enformer\u2019s training parameters and architecture were not a primary focus of this work. Therefore, there are potential gains to be made on our results. The learning rate was held constant at 0.0001 during training. Alternative rates, or starting with a higher rate and decaying it, could result in faster training and improved performance. The default model architecture values were used to train S-Enformer. With respect to the parameters for the sparse-attention layers, values that could be changed include the block size, number of random blocks, number of sliding blocks, and number of global blocks. Each of these values will affect the amount of attention used by the model. Given the complexities of this task, it is possible that the model is not attending to enough of the data and increasing some of these values could be beneficial. More transformer layers may be required In a self-attention layer, all nodes are connected with each other. This allows information to easily be shared across the neural network. In BigBird\u2019s sparse-attention, not all nodes are connected within a layer. For two unconnected nodes in a layer to share information, they must pass information via their connections with nodes in other layers. Given the large input sequence length, more layers than the currently used eleven could be required to achieve accuracy parity with Enformer. As a default, Bigbird uses sixteen transformer layers, which suggests that more layers might be required for optimal performance 1 . Depending on the number of additional transformer layers required to match Enformer\u2019s performance, the benefits of sparse-attention could deteriorate. Increase prediction resolution Enformer was designed to predict human and mouse genomic tracks at 128 bp resolution. Depending on the task, it could be beneficial to have a more precise resolution, such as to a single base. BPNet is one such model that was designed for nucleotide-resolution prediction 6 . Differing from S-Enformer, BPNet was trained with single-cell data, instead of bulk sequencing data. Its input sequence length is 2,068 bp, which is longer than if Enformer was scaled down to the same resolution (1,538 = 196,608 / 128). However, Enformer should be able to use a longer input sequence than 1,538 bp since shorter sequences use less memory. Nonetheless, using S-Enformer\u2019s architecture should allow for a much longer input sequence. This is because BPNet 6 uses dilated convolutional layers, which were also used by Basenji 4 , the prior work to Enformer. Enformer has already demonstrated that using transformers instead of dilated convolutional layers allows for a longer input sequence length 3 . The addition of sparse-attention should further these gains. A nucleotide-resolution model could be applied to GWAS to help identify the causal variant(s) for a disease or trait. This is due to the unidirectional flow of information in a model, meaning that a mutation in the input DNA sequence is the single reason for a change in the predicted gene expression. Contrary to GWAS, where there is only an association between a mutation and disease or trait. Therefore, the high-resolution predictions of a model can complement the wide search space of GWAS to better identify nucleotides of interest. Alternative sparse-attention methods There are multiple other sparse-attention methods that are worth exploring 2 . Two notable ones are the Linear Transformer 7 and Performer 8 . Both of these models train faster and have a lower peak memory usage than BigBird 2 . Since Enformer was trained for three days using 64 TPUs, reducing these training resources is critical for model retraining with sparse-attention to be more accessible. When looking at the average accuracy of predictions, Bigbird is the best performing model, but the more relevant metric is likely the text classification results 2 , as this is the most similar task to predicting gene expression. In the text classification task, both the Linear Transformer and Performer outperform Bigbird, which suggests that they could achieve better results in predicting gene expression 2 . Zaheer, Manzil, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, et al. 2021. \u2018Big Bird: Transformers for Longer Sequences\u2019. ArXiv:2007.14062 [Cs, Stat], January. http://arxiv.org/abs/2007.14062 . \u21a9 \u21a9 Tay, Yi, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2020. \u2018Long Range Arena: A Benchmark for Efficient Transformers\u2019. ArXiv:2011.04006 [Cs], November. http://arxiv.org/abs/2011.04006 . \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 Avsec, \u017diga, Vikram Agarwal, Daniel Visentin, Joseph R. Ledsam, Agnieszka Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R. Kelley. 2021. \u2018Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions\u2019. Nature Methods 18 (10): 1196\u20131203. https://doi.org/10.1038/s41592-021-01252-x . \u21a9 \u21a9 Kelley, David R., Yakir A. Reshef, Maxwell Bileschi, David Belanger, Cory Y. McLean, and Jasper Snoek. 2018. \u2018Sequential Regulatory Activity Prediction across Chromosomes with Convolutional Neural Networks\u2019. Genome Research 28 (5): 739\u201350. https://doi.org/10.1101/gr.227819.117 . \u21a9 \u21a9 Zhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K. Wong, and Olga G. Troyanskaya. 2018. \u2018Deep Learning Sequence-Based Ab Initio Prediction of Variant Effects on Expression and Disease Risk\u2019. Nature Genetics 50 (8): 1171\u201379. https://doi.org/10.1038/s41588-018-0160-6 . \u21a9 Avsec, \u017diga, Melanie Weilert, Avanti Shrikumar, Sabrina Krueger, Amr Alexandari, Khyati Dalal, Robin Fropf, et al. 2021. \u2018Base-Resolution Models of Transcription-Factor Binding Reveal Soft Motif Syntax\u2019. Nature Genetics 53 (3): 354\u201366. https://doi.org/10.1038/s41588-021-00782-6 . \u21a9 \u21a9 Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. 2020. \u2018Transformers Are RNNs: Fast Autoregressive Transformers with Linear Attention\u2019. ArXiv:2006.16236 [Cs, Stat], August. http://arxiv.org/abs/2006.16236 . \u21a9 Choromanski, Krzysztof, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, et al. 2021. \u2018Rethinking Attention with Performers\u2019. ArXiv:2009.14794 [Cs, Stat], March. http://arxiv.org/abs/2009.14794 . \u21a9","title":"Discussion"},{"location":"discussion/#sparse-attention-should-lead-to-more-accurate-predictions","text":"Deep learning models have made significant gains in predicting gene expression from a sequence of DNA. A key limitation of these models is that they still lack the ability to use long enough sequences to include all regulatory elements in their predictions. By replacing the self-attention mechanism from the Enformer model with BigBird\u2019s sparse-attention, we have taken one step closer to overcoming this obstacle. Although we lacked the time and computational resources to train the model from scratch and demonstrate that using sparse-attention and a longer input sequence will achieve more accurate results, we are confident that this will be the case. This confidence is based on the state-of-the-art results achieved by BigBird 1 , 2 from its novel architecture and earlier models scoring greater accuracy through the use of longer input sequences 3 , 4 , 5 .","title":"Sparse-attention should lead to more accurate predictions"},{"location":"discussion/#limitations-to-model-training","text":"A possible explanation for not achieving accuracy parity with Enformer is the constraint placed on the model during training. When a model is being trained from scratch, all of its weights update in order to reduce the loss. This allows the model to make the best use of its architecture. When training S-Enformer, the model could not update in a cohesive way as all of the weights were frozen except for those in the sparse-attention layers. The sparse-attention layers\u2019 weights were forced to update in a constrained environment and possibly not make optimal use of their design. Given that the self-attention layers have a different architecture to the sparse-attention layers, it might not be possible for the model to achieve its optimal performance when only the sparse-attention layer weights can update.","title":"Limitations to model training"},{"location":"discussion/#parameter-optimisation-could-improve-results","text":"Optimising S-Enformer\u2019s training parameters and architecture were not a primary focus of this work. Therefore, there are potential gains to be made on our results. The learning rate was held constant at 0.0001 during training. Alternative rates, or starting with a higher rate and decaying it, could result in faster training and improved performance. The default model architecture values were used to train S-Enformer. With respect to the parameters for the sparse-attention layers, values that could be changed include the block size, number of random blocks, number of sliding blocks, and number of global blocks. Each of these values will affect the amount of attention used by the model. Given the complexities of this task, it is possible that the model is not attending to enough of the data and increasing some of these values could be beneficial.","title":"Parameter optimisation could improve results"},{"location":"discussion/#more-transformer-layers-may-be-required","text":"In a self-attention layer, all nodes are connected with each other. This allows information to easily be shared across the neural network. In BigBird\u2019s sparse-attention, not all nodes are connected within a layer. For two unconnected nodes in a layer to share information, they must pass information via their connections with nodes in other layers. Given the large input sequence length, more layers than the currently used eleven could be required to achieve accuracy parity with Enformer. As a default, Bigbird uses sixteen transformer layers, which suggests that more layers might be required for optimal performance 1 . Depending on the number of additional transformer layers required to match Enformer\u2019s performance, the benefits of sparse-attention could deteriorate.","title":"More transformer layers may be required"},{"location":"discussion/#increase-prediction-resolution","text":"Enformer was designed to predict human and mouse genomic tracks at 128 bp resolution. Depending on the task, it could be beneficial to have a more precise resolution, such as to a single base. BPNet is one such model that was designed for nucleotide-resolution prediction 6 . Differing from S-Enformer, BPNet was trained with single-cell data, instead of bulk sequencing data. Its input sequence length is 2,068 bp, which is longer than if Enformer was scaled down to the same resolution (1,538 = 196,608 / 128). However, Enformer should be able to use a longer input sequence than 1,538 bp since shorter sequences use less memory. Nonetheless, using S-Enformer\u2019s architecture should allow for a much longer input sequence. This is because BPNet 6 uses dilated convolutional layers, which were also used by Basenji 4 , the prior work to Enformer. Enformer has already demonstrated that using transformers instead of dilated convolutional layers allows for a longer input sequence length 3 . The addition of sparse-attention should further these gains. A nucleotide-resolution model could be applied to GWAS to help identify the causal variant(s) for a disease or trait. This is due to the unidirectional flow of information in a model, meaning that a mutation in the input DNA sequence is the single reason for a change in the predicted gene expression. Contrary to GWAS, where there is only an association between a mutation and disease or trait. Therefore, the high-resolution predictions of a model can complement the wide search space of GWAS to better identify nucleotides of interest.","title":"Increase prediction resolution"},{"location":"discussion/#alternative-sparse-attention-methods","text":"There are multiple other sparse-attention methods that are worth exploring 2 . Two notable ones are the Linear Transformer 7 and Performer 8 . Both of these models train faster and have a lower peak memory usage than BigBird 2 . Since Enformer was trained for three days using 64 TPUs, reducing these training resources is critical for model retraining with sparse-attention to be more accessible. When looking at the average accuracy of predictions, Bigbird is the best performing model, but the more relevant metric is likely the text classification results 2 , as this is the most similar task to predicting gene expression. In the text classification task, both the Linear Transformer and Performer outperform Bigbird, which suggests that they could achieve better results in predicting gene expression 2 . Zaheer, Manzil, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, et al. 2021. \u2018Big Bird: Transformers for Longer Sequences\u2019. ArXiv:2007.14062 [Cs, Stat], January. http://arxiv.org/abs/2007.14062 . \u21a9 \u21a9 Tay, Yi, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2020. \u2018Long Range Arena: A Benchmark for Efficient Transformers\u2019. ArXiv:2011.04006 [Cs], November. http://arxiv.org/abs/2011.04006 . \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 Avsec, \u017diga, Vikram Agarwal, Daniel Visentin, Joseph R. Ledsam, Agnieszka Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R. Kelley. 2021. \u2018Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions\u2019. Nature Methods 18 (10): 1196\u20131203. https://doi.org/10.1038/s41592-021-01252-x . \u21a9 \u21a9 Kelley, David R., Yakir A. Reshef, Maxwell Bileschi, David Belanger, Cory Y. McLean, and Jasper Snoek. 2018. \u2018Sequential Regulatory Activity Prediction across Chromosomes with Convolutional Neural Networks\u2019. Genome Research 28 (5): 739\u201350. https://doi.org/10.1101/gr.227819.117 . \u21a9 \u21a9 Zhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K. Wong, and Olga G. Troyanskaya. 2018. \u2018Deep Learning Sequence-Based Ab Initio Prediction of Variant Effects on Expression and Disease Risk\u2019. Nature Genetics 50 (8): 1171\u201379. https://doi.org/10.1038/s41588-018-0160-6 . \u21a9 Avsec, \u017diga, Melanie Weilert, Avanti Shrikumar, Sabrina Krueger, Amr Alexandari, Khyati Dalal, Robin Fropf, et al. 2021. \u2018Base-Resolution Models of Transcription-Factor Binding Reveal Soft Motif Syntax\u2019. Nature Genetics 53 (3): 354\u201366. https://doi.org/10.1038/s41588-021-00782-6 . \u21a9 \u21a9 Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. 2020. \u2018Transformers Are RNNs: Fast Autoregressive Transformers with Linear Attention\u2019. ArXiv:2006.16236 [Cs, Stat], August. http://arxiv.org/abs/2006.16236 . \u21a9 Choromanski, Krzysztof, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, et al. 2021. \u2018Rethinking Attention with Performers\u2019. ArXiv:2009.14794 [Cs, Stat], March. http://arxiv.org/abs/2009.14794 . \u21a9","title":"Alternative sparse-attention methods"},{"location":"methods/","text":"Model architecture The S-Enformer architecture is a combination of the Enformer 1 and BigBird 2 models. At a high level, S-Enformer is composed of the six parts ( Figure 9 ): (1: Stem ) convolution blocks with attention pooling, (2: Convolutional tower ) 6 convolutional blocks with batch normalisation and attention pooling, (3: Transformers ) 11 transformer blocks with layer normalisation, sparse-attention, and dropout, (4: Cropping ) a cropping layer, (5: Pointwise convolution ) a pointwise convolution with dropout and a Gaussian error linear unit (GELU) activation function, and (6: Heads ) a network head for each organism consisting of a dense layer and softplus activation function. Tensorflow v2.4.1 3 and Sonnet v2.0.0 4 were the primary libraries for developing the model. Figure 9: Model architecture The architecture of the Enformer and S-Enformer models. The image above is largely based on Extended Figure 1 from the Enformer paper. Given that the only architectural difference between Enformer and S-Enformer is the type of attention, the same diagram can be used to illustrate both models. The lower set of flowcharts provide greater detail about some of the components in the upper image. The input to S-Enformer is a 3-D matrix with dimensions for batch size, sequence length, and nucleotides. The batch size was set to 5, but it can vary based on the training requirements, the sequence length is always 196,608 bp, and there are 4 nucleotides (corresponding to A = [1,0,0,0], C = [0,1,0,0], G = [0,0,1,0], T = [0,0,0,1], N = [0,0,0,0]). When an input sequence is fed into S-Enformer, it is first reduced in length to 1,536 by the attention pooling in the stem and convolutional tower. Then, distal interactions within the sequence are learned by the transformer blocks. The cropping layer removes 320 positions at both ends of the sequence as the Enformer model could not share information to these edges from the opposite half of the sequence 1 . It is possible for this layer to be removed so that more nodes connect with the pointwise convolutional layer and potentially increase the accuracy of the model, but changing the model architecture this significantly was outside the scope of this work. In the last section of the model, it predicts the genomic tracks for the relevant organism. There are 5,313 tracks for humans and 1,643 for mice, each of length 896, which correspond to a DNA sequence length of 114,688 bp aggregated into 128 bp bins. Sparse attention The multi-head attention layers help the transformer blocks to learn the relationship between DNA sequence and expression levels. This is done by having each head (eight were used in S-Enformer) attend to the sequence separately, thereby learning different relationships between the DNA sequence and expression level. The output of these heads are concatenated and linearly transformed to be used as input by the next step in the transformer block 5 . Within each attention head, it is the sparse-attention mechanism that allows the model to learn these relationships with a linear (as opposed to quadratic) dependency on memory. There are three components to sparse-attention: (1) random attention, (2) sliding attention, and (3) global attention ( Figure 10 ). Figure 10: The three components of sparse-attention This diagram illustrates which tokens are attended to when using sparse-attention. Global attention is used by the first and last token as they are connected to all other tokens. Sliding attention is when a token attends to its neighbouring tokens. Random attention is used by random tokens attending to other ones. They are shown in pairs to demonstrate the use of blocks by BigBird\u2019s sparse-attention mechanism, i.e. groups of tokens attend to each other, rather than individual tokens. This is a simplified diagram of S-Enformer\u2019s sparse-attention as each square represents 64 tokens, which is the block size of the model. Random attention is when a subset of tokens attend to other tokens. This allows information to be shared across the network, but without the memory requirements of full attention. 192 tokens (of the 1,536) are randomly attended to in each layer. Sliding attention makes use of the locality of reference . In a DNA sequence (and natural language processing in general), a significant amount of information can be learned about a token from its neighbouring tokens 2 . In S-Enformer, a token attends to 64 tokens to the left and right of itself to capture the local information. Global attention is when a token attends to all other tokens in a sequence. For S-Enformer, global attention is used by the first and last 64 tokens of a sequence. In comparison, all tokens use global attention in Enformer, therefore it can be another term for self-attention. The benefit of global attention is that it can help information to move much faster in a network than relying on just sliding or random attention, as global tokens are connected with all other tokens. An issue with this attention is that sparse multiplication is inefficient when training a model on a GPU 6 , 7 . To overcome this, query tokens and key tokens are grouped together into attention blocks (instead of being uniformly distributed). This allows matrix multiplication to be used, rather than a gather operation, which is much faster. Since sparse-attention is being used, the time complexity is linear, O(nbd) (n = number of tokens, b = block size, d = number of tokens to attend to), instead of quadratic, O(n2d), for self-attention 2 . This results in fewer than half of nodes being attended to in each layer using sparse-attention, compared to all nodes attending to all others in each layer using self-attention. Modelling data S-Enformer was trained, validated, and tested using the same data as Enformer 1 . There are 34,021 training, 2,213 validation, and 1,937 test sequences for the human genome, and 29,295 training, 2,209 validation, and 2,017 test sequences for the mouse genome. For the human genome, each example contains 684 DNase-seq or ATAC-seq, 1,860 histone modification ChIP\u2013Seq, 2,131 TF ChIP-Seq, and 638 CAGE tracks (totalling 5,313). For the mouse genome, each example contains 228 DNase-seq or ATAC-Seq, 750 histone modification ChIP\u2013Seq, 308 TF ChIP\u2013Seq, and 357 CAGE tracks (totalling 1,643). Model training The model was trained for 25,000 steps using a batch size of 5 on a single RTX 6000 GPU, which took about 3 days ( link to code ). The Adam optimizer with a learning rate of 0.0001 was used to train S-Enformer, which is less than the final learning rate of 0.0005 for Enformer. The higher rate used by Enformer resulted in lower Pearson correlation coefficients for S-Enformer on the training and validation data. The same architecture parameters were used as Enformer, specifically 1,536 channels, 8 attention heads, and 11 transformer blocks. A pre-trained Enformer model, shared by its authors, was used when comparing the performance against S-Enformer. Memory usage and training speed A model\u2019s memory usage and training speed was measured across 10 training steps using a batch size of 1 on a single RTX 6000 GPU ( link to code ). The peak memory usage was reported using the tf.config.experimental.get_memory_info function. After each training step, the memory stats were reset so that an average could be taken across the first 10 steps. Measuring the training time began just before the first training step and concluded once the tenth training step had finished. Accuracy evaluation Only the human testing dataset was used to compare the performance between the two models ( link to code ). The mouse testing data was omitted as the human data was sufficient to make a fair comparison and we are more interested in these results. The Pearson correlation coefficient was reported for each of the four genomic tracks: (1) DNase-Seq and ATAC-Seq, (2) histone modifications ChIP-Seq, (3) TF ChIP-Seq, and (4) CAGE. Supplementary Table 2 from the Enformer paper was used to label the track type within each of the 5,313 testing examples. The mean Pearson correlation coefficient was reported for each track type to compare the performance between Enformer and S-Enformer. Measuring the correlation of predictions The correlation between Enformer\u2019s and S-Enformer\u2019s predictions were measured using the 1,939 testing sequences (the workflow is illustrated in Figure 11 ; link to code ). Each sequence was inputted into the two models to collect the gene expression for the 5,313 genomic tracks. The tracks were then grouped into their four genomic track types and had their (Pearson) correlation measured with the experimental results to evaluate their quality. For each of the 1,939 testing sequences, the correlations were averaged within the four track types. The two model\u2019s averaged-correlation values were then correlated (Pearson) with each other to understand the similarity between predictions. Figure 11: Workflow to measure the correlation of predictions The workflow to measure the correlation of predictions from Enformer and S-Enformer within the four genomic track types. First, the testing sequences are fed into the models to predict gene expression. Then the predictions are grouped into their genomic track types before being correlated with the experiment results. The correlations are then averaged within each of the 1,939 testing examples. Finally the averaged correlation coefficients for the two models are correlated with each other to help understand the similarity of predictions. To determine how often S-Enformer outperformed Enformer, the averaged-correlation values were compared, within each track type, to measure the percentage of the testing examples that S-Enformer had a higher average correlation than Enformer. Receptive field The receptive field for a model was measured by: (1) taking a random sequence of DNA (length = 196,608), (2) predicting the expression across all 5,313 genomic tracks, (3) randomly changing a single base at a predefined location (such as the first nucleotide), (4) again predicting the expression across all 5,313 genomic tracks, (5) calculating the difference in expression between the two sets of predictions, (6) repeating steps 1-5 one hundred times, and (7) averaging the change in expression for the 896 position across the 5,313 genomic tracks. These seven steps were repeated nine times, each using a different mutation location. The nine locations were equally spaced across the sequence of DNA ( link to code ). Avsec, \u017diga, Vikram Agarwal, Daniel Visentin, Joseph R. Ledsam, Agnieszka Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R. Kelley. 2021. \u2018Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions\u2019. Nature Methods 18 (10): 1196\u20131203. https://doi.org/10.1038/s41592-021-01252-x . \u21a9 \u21a9 \u21a9 Zaheer, Manzil, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, et al. 2021. \u2018Big Bird: Transformers for Longer Sequences\u2019. ArXiv:2007.14062 [Cs, Stat], January. http://arxiv.org/abs/2007.14062 . \u21a9 \u21a9 \u21a9 TensorFlow Developers. 2022. TensorFlow (version v2.8.2). Zenodo. https://doi.org/10.5281/ZENODO.4724125 . \u21a9 Sonnet Developers. 2020. Sonnet Documentation \u2014 Sonnet Documentation (version v2.0.0). https://sonnet.readthedocs.io/en/latest/ . \u21a9 Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. \u2018Attention Is All You Need\u2019. ArXiv:1706.03762 [Cs], December. http://arxiv.org/abs/1706.03762 . \u21a9 Gale, Trevor, Matei Zaharia, Cliff Young, and Erich Elsen. 2020. \u2018Sparse GPU Kernels for Deep Learning\u2019. ArXiv:2006.10901 [Cs, Stat], August. http://arxiv.org/abs/2006.10901 . \u21a9 Yao, Zhuliang, Shijie Cao, Wencong Xiao, Chen Zhang, and Lanshun Nie. 2019. \u2018Balanced Sparsity for Efficient DNN Inference on GPU\u2019. Proceedings of the AAAI Conference on Artificial Intelligence 33 (July): 5676\u201383. https://doi.org/10.1609/aaai.v33i01.33015676 . \u21a9","title":"Methods"},{"location":"methods/#model-architecture","text":"The S-Enformer architecture is a combination of the Enformer 1 and BigBird 2 models. At a high level, S-Enformer is composed of the six parts ( Figure 9 ): (1: Stem ) convolution blocks with attention pooling, (2: Convolutional tower ) 6 convolutional blocks with batch normalisation and attention pooling, (3: Transformers ) 11 transformer blocks with layer normalisation, sparse-attention, and dropout, (4: Cropping ) a cropping layer, (5: Pointwise convolution ) a pointwise convolution with dropout and a Gaussian error linear unit (GELU) activation function, and (6: Heads ) a network head for each organism consisting of a dense layer and softplus activation function. Tensorflow v2.4.1 3 and Sonnet v2.0.0 4 were the primary libraries for developing the model.","title":"Model architecture"},{"location":"methods/#figure-9-model-architecture","text":"The architecture of the Enformer and S-Enformer models. The image above is largely based on Extended Figure 1 from the Enformer paper. Given that the only architectural difference between Enformer and S-Enformer is the type of attention, the same diagram can be used to illustrate both models. The lower set of flowcharts provide greater detail about some of the components in the upper image. The input to S-Enformer is a 3-D matrix with dimensions for batch size, sequence length, and nucleotides. The batch size was set to 5, but it can vary based on the training requirements, the sequence length is always 196,608 bp, and there are 4 nucleotides (corresponding to A = [1,0,0,0], C = [0,1,0,0], G = [0,0,1,0], T = [0,0,0,1], N = [0,0,0,0]). When an input sequence is fed into S-Enformer, it is first reduced in length to 1,536 by the attention pooling in the stem and convolutional tower. Then, distal interactions within the sequence are learned by the transformer blocks. The cropping layer removes 320 positions at both ends of the sequence as the Enformer model could not share information to these edges from the opposite half of the sequence 1 . It is possible for this layer to be removed so that more nodes connect with the pointwise convolutional layer and potentially increase the accuracy of the model, but changing the model architecture this significantly was outside the scope of this work. In the last section of the model, it predicts the genomic tracks for the relevant organism. There are 5,313 tracks for humans and 1,643 for mice, each of length 896, which correspond to a DNA sequence length of 114,688 bp aggregated into 128 bp bins.","title":"Figure 9: Model architecture"},{"location":"methods/#sparse-attention","text":"The multi-head attention layers help the transformer blocks to learn the relationship between DNA sequence and expression levels. This is done by having each head (eight were used in S-Enformer) attend to the sequence separately, thereby learning different relationships between the DNA sequence and expression level. The output of these heads are concatenated and linearly transformed to be used as input by the next step in the transformer block 5 . Within each attention head, it is the sparse-attention mechanism that allows the model to learn these relationships with a linear (as opposed to quadratic) dependency on memory. There are three components to sparse-attention: (1) random attention, (2) sliding attention, and (3) global attention ( Figure 10 ).","title":"Sparse attention"},{"location":"methods/#figure-10-the-three-components-of-sparse-attention","text":"This diagram illustrates which tokens are attended to when using sparse-attention. Global attention is used by the first and last token as they are connected to all other tokens. Sliding attention is when a token attends to its neighbouring tokens. Random attention is used by random tokens attending to other ones. They are shown in pairs to demonstrate the use of blocks by BigBird\u2019s sparse-attention mechanism, i.e. groups of tokens attend to each other, rather than individual tokens. This is a simplified diagram of S-Enformer\u2019s sparse-attention as each square represents 64 tokens, which is the block size of the model. Random attention is when a subset of tokens attend to other tokens. This allows information to be shared across the network, but without the memory requirements of full attention. 192 tokens (of the 1,536) are randomly attended to in each layer. Sliding attention makes use of the locality of reference . In a DNA sequence (and natural language processing in general), a significant amount of information can be learned about a token from its neighbouring tokens 2 . In S-Enformer, a token attends to 64 tokens to the left and right of itself to capture the local information. Global attention is when a token attends to all other tokens in a sequence. For S-Enformer, global attention is used by the first and last 64 tokens of a sequence. In comparison, all tokens use global attention in Enformer, therefore it can be another term for self-attention. The benefit of global attention is that it can help information to move much faster in a network than relying on just sliding or random attention, as global tokens are connected with all other tokens. An issue with this attention is that sparse multiplication is inefficient when training a model on a GPU 6 , 7 . To overcome this, query tokens and key tokens are grouped together into attention blocks (instead of being uniformly distributed). This allows matrix multiplication to be used, rather than a gather operation, which is much faster. Since sparse-attention is being used, the time complexity is linear, O(nbd) (n = number of tokens, b = block size, d = number of tokens to attend to), instead of quadratic, O(n2d), for self-attention 2 . This results in fewer than half of nodes being attended to in each layer using sparse-attention, compared to all nodes attending to all others in each layer using self-attention.","title":"Figure 10: The three components of sparse-attention"},{"location":"methods/#modelling-data","text":"S-Enformer was trained, validated, and tested using the same data as Enformer 1 . There are 34,021 training, 2,213 validation, and 1,937 test sequences for the human genome, and 29,295 training, 2,209 validation, and 2,017 test sequences for the mouse genome. For the human genome, each example contains 684 DNase-seq or ATAC-seq, 1,860 histone modification ChIP\u2013Seq, 2,131 TF ChIP-Seq, and 638 CAGE tracks (totalling 5,313). For the mouse genome, each example contains 228 DNase-seq or ATAC-Seq, 750 histone modification ChIP\u2013Seq, 308 TF ChIP\u2013Seq, and 357 CAGE tracks (totalling 1,643).","title":"Modelling data"},{"location":"methods/#model-training","text":"The model was trained for 25,000 steps using a batch size of 5 on a single RTX 6000 GPU, which took about 3 days ( link to code ). The Adam optimizer with a learning rate of 0.0001 was used to train S-Enformer, which is less than the final learning rate of 0.0005 for Enformer. The higher rate used by Enformer resulted in lower Pearson correlation coefficients for S-Enformer on the training and validation data. The same architecture parameters were used as Enformer, specifically 1,536 channels, 8 attention heads, and 11 transformer blocks. A pre-trained Enformer model, shared by its authors, was used when comparing the performance against S-Enformer.","title":"Model training"},{"location":"methods/#memory-usage-and-training-speed","text":"A model\u2019s memory usage and training speed was measured across 10 training steps using a batch size of 1 on a single RTX 6000 GPU ( link to code ). The peak memory usage was reported using the tf.config.experimental.get_memory_info function. After each training step, the memory stats were reset so that an average could be taken across the first 10 steps. Measuring the training time began just before the first training step and concluded once the tenth training step had finished.","title":"Memory usage and training speed"},{"location":"methods/#accuracy-evaluation","text":"Only the human testing dataset was used to compare the performance between the two models ( link to code ). The mouse testing data was omitted as the human data was sufficient to make a fair comparison and we are more interested in these results. The Pearson correlation coefficient was reported for each of the four genomic tracks: (1) DNase-Seq and ATAC-Seq, (2) histone modifications ChIP-Seq, (3) TF ChIP-Seq, and (4) CAGE. Supplementary Table 2 from the Enformer paper was used to label the track type within each of the 5,313 testing examples. The mean Pearson correlation coefficient was reported for each track type to compare the performance between Enformer and S-Enformer.","title":"Accuracy evaluation"},{"location":"methods/#measuring-the-correlation-of-predictions","text":"The correlation between Enformer\u2019s and S-Enformer\u2019s predictions were measured using the 1,939 testing sequences (the workflow is illustrated in Figure 11 ; link to code ). Each sequence was inputted into the two models to collect the gene expression for the 5,313 genomic tracks. The tracks were then grouped into their four genomic track types and had their (Pearson) correlation measured with the experimental results to evaluate their quality. For each of the 1,939 testing sequences, the correlations were averaged within the four track types. The two model\u2019s averaged-correlation values were then correlated (Pearson) with each other to understand the similarity between predictions.","title":"Measuring the correlation of predictions"},{"location":"methods/#figure-11-workflow-to-measure-the-correlation-of-predictions","text":"The workflow to measure the correlation of predictions from Enformer and S-Enformer within the four genomic track types. First, the testing sequences are fed into the models to predict gene expression. Then the predictions are grouped into their genomic track types before being correlated with the experiment results. The correlations are then averaged within each of the 1,939 testing examples. Finally the averaged correlation coefficients for the two models are correlated with each other to help understand the similarity of predictions. To determine how often S-Enformer outperformed Enformer, the averaged-correlation values were compared, within each track type, to measure the percentage of the testing examples that S-Enformer had a higher average correlation than Enformer.","title":"Figure 11: Workflow to measure the correlation of predictions"},{"location":"methods/#receptive-field","text":"The receptive field for a model was measured by: (1) taking a random sequence of DNA (length = 196,608), (2) predicting the expression across all 5,313 genomic tracks, (3) randomly changing a single base at a predefined location (such as the first nucleotide), (4) again predicting the expression across all 5,313 genomic tracks, (5) calculating the difference in expression between the two sets of predictions, (6) repeating steps 1-5 one hundred times, and (7) averaging the change in expression for the 896 position across the 5,313 genomic tracks. These seven steps were repeated nine times, each using a different mutation location. The nine locations were equally spaced across the sequence of DNA ( link to code ). Avsec, \u017diga, Vikram Agarwal, Daniel Visentin, Joseph R. Ledsam, Agnieszka Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R. Kelley. 2021. \u2018Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions\u2019. Nature Methods 18 (10): 1196\u20131203. https://doi.org/10.1038/s41592-021-01252-x . \u21a9 \u21a9 \u21a9 Zaheer, Manzil, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, et al. 2021. \u2018Big Bird: Transformers for Longer Sequences\u2019. ArXiv:2007.14062 [Cs, Stat], January. http://arxiv.org/abs/2007.14062 . \u21a9 \u21a9 \u21a9 TensorFlow Developers. 2022. TensorFlow (version v2.8.2). Zenodo. https://doi.org/10.5281/ZENODO.4724125 . \u21a9 Sonnet Developers. 2020. Sonnet Documentation \u2014 Sonnet Documentation (version v2.0.0). https://sonnet.readthedocs.io/en/latest/ . \u21a9 Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. \u2018Attention Is All You Need\u2019. ArXiv:1706.03762 [Cs], December. http://arxiv.org/abs/1706.03762 . \u21a9 Gale, Trevor, Matei Zaharia, Cliff Young, and Erich Elsen. 2020. \u2018Sparse GPU Kernels for Deep Learning\u2019. ArXiv:2006.10901 [Cs, Stat], August. http://arxiv.org/abs/2006.10901 . \u21a9 Yao, Zhuliang, Shijie Cao, Wencong Xiao, Chen Zhang, and Lanshun Nie. 2019. \u2018Balanced Sparsity for Efficient DNN Inference on GPU\u2019. Proceedings of the AAAI Conference on Artificial Intelligence 33 (July): 5676\u201383. https://doi.org/10.1609/aaai.v33i01.33015676 . \u21a9","title":"Receptive field"},{"location":"references/","text":"Aguet, Fran\u00e7ois, Andrew A. Brown, Stephane E. Castel, Joe R. Davis, Yuan He, Brian Jo, Pejman Mohammadi, et al. 2017. \u2018Genetic Effects on Gene Expression across Human Tissues\u2019. Nature 550 (7675): 204\u201313. https://doi.org/10.1038/nature24277 . Albert, Frank W., and Leonid Kruglyak. 2015. \u2018The Role of Regulatory Variation in Complex Traits and Disease\u2019. Nature Reviews Genetics 16 (4): 197\u2013212. https://doi.org/10.1038/nrg3891 . Avsec, \u017diga, Vikram Agarwal, Daniel Visentin, Joseph R. Ledsam, Agnieszka Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R. Kelley. 2021. \u2018Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions\u2019. Nature Methods 18 (10): 1196\u20131203. https://doi.org/10.1038/s41592-021-01252-x . Avsec, \u017diga, Melanie Weilert, Avanti Shrikumar, Sabrina Krueger, Amr Alexandari, Khyati Dalal, Robin Fropf, et al. 2021. \u2018Base-Resolution Models of Transcription-Factor Binding Reveal Soft Motif Syntax\u2019. Nature Genetics 53 (3): 354\u201366. https://doi.org/10.1038/s41588-021-00782-6 . Choromanski, Krzysztof, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, et al. 2021. \u2018Rethinking Attention with Performers\u2019. ArXiv:2009.14794 [Cs, Stat], March. http://arxiv.org/abs/2009.14794 . Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. \u2018BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding\u2019. ArXiv:1810.04805 [Cs], May. http://arxiv.org/abs/1810.04805 . Edwards, Stacey L., Jonathan Beesley, Juliet D. French, and Alison M. Dunning. 2013. \u2018Beyond GWASs: Illuminating the Dark Road from Association to Function\u2019. The American Journal of Human Genetics 93 (5): 779\u201397. https://doi.org/10.1016/j.ajhg.2013.10.012 . Forrest et al. 2014. \u2018A Promoter-Level Mammalian Expression Atlas\u2019. Nature 507 (7493): 462. https://doi.org/10.1038/nature13182 . Gale, Trevor, Matei Zaharia, Cliff Young, and Erich Elsen. 2020. \u2018Sparse GPU Kernels for Deep Learning\u2019. ArXiv:2006.10901 [Cs, Stat], August. http://arxiv.org/abs/2006.10901 . Hobert, Oliver. 2008. \u2018Gene Regulation by Transcription Factors and MicroRNAs\u2019. Science 319 (5871): 1785\u201386. https://doi.org/10.1126/science.1151651 . Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. 2020. \u2018Transformers Are RNNs: Fast Autoregressive Transformers with Linear Attention\u2019. ArXiv:2006.16236 [Cs, Stat], August. http://arxiv.org/abs/2006.16236 . Kelley, David R. 2020. \u2018Cross-Species Regulatory Sequence Activity Prediction\u2019. PLOS Computational Biology 16 (7): e1008050. https://doi.org/10.1371/journal.pcbi.1008050 . Kelley, David R., Yakir A. Reshef, Maxwell Bileschi, David Belanger, Cory Y. McLean, and Jasper Snoek. 2018. \u2018Sequential Regulatory Activity Prediction across Chromosomes with Convolutional Neural Networks\u2019. Genome Research 28 (5): 739\u201350. https://doi.org/10.1101/gr.227819.117 . Kelley, David R., Jasper Snoek, and John L. Rinn. 2016. \u2018Basset: Learning the Regulatory Code of the Accessible Genome with Deep Convolutional Neural Networks\u2019. Genome Research 26 (7): 990\u201399. https://doi.org/10.1101/gr.200535.115 . Kitaev, Nikita, \u0141ukasz Kaiser, and Anselm Levskaya. 2020. \u2018Reformer: The Efficient Transformer\u2019. ArXiv:2001.04451 [Cs, Stat], February. http://arxiv.org/abs/2001.04451 . Krivega, Ivan, and Ann Dean. 2012. \u2018Enhancer and Promoter Interactions \u2014 Long Distance Calls\u2019. Current Opinion in Genetics & Development 22 (2): 79. https://doi.org/10.1016/j.gde.2011.11.001 . Leslie, R., C. J. O\u2019Donnell, and A. D. Johnson. 2014. \u2018GRASP: Analysis of Genotype-Phenotype Results from 1390 Genome-Wide Association Studies and Corresponding Open Access Database\u2019. Bioinformatics 30 (12): i185\u201394. https://doi.org/10.1093/bioinformatics/btu273 . Levine, Mike. 2010. \u2018Transcriptional Enhancers in Animal Development and Evolution\u2019. Current Biology 20 (17): R754\u201363. https://doi.org/10.1016/j.cub.2010.06.070 . Long, Hannah K., Sara L. Prescott, and Joanna Wysocka. 2016. \u2018Ever-Changing Landscapes: Transcriptional Enhancers in Development and Evolution\u2019. Cell 167 (5): 1170\u201387. https://doi.org/10.1016/j.cell.2016.09.018 . Mamoshina, Polina, Armando Vieira, Evgeny Putin, and Alex Zhavoronkov. 2016. \u2018Applications of Deep Learning in Biomedicine\u2019. Molecular Pharmaceutics 13 (5): 1445\u201354. https://doi.org/10.1021/acs.molpharmaceut.5b00982 . Pei, Guangsheng, Ruifeng Hu, Yulin Dai, Astrid Marilyn Manuel, Zhongming Zhao, and Peilin Jia. 2021. \u2018Predicting Regulatory Variants Using a Dense Epigenomic Mapped CNN Model Elucidated the Molecular Basis of Trait-Tissue Associations\u2019. Nucleic Acids Research 49 (1): 53\u201366. https://doi.org/10.1093/nar/gkaa1137 . Pennacchio, Len A., Nadav Ahituv, Alan M. Moses, Shyam Prabhakar, Marcelo A. Nobrega, Malak Shoukry, Simon Minovitsky, et al. 2006. \u2018In Vivo Enhancer Analysis of Human Conserved Non-Coding Sequences\u2019. Nature 444 (7118): 499\u2013502. https://doi.org/10.1038/nature05295 . Roadmap Epigenomics Consortium, Wouter Kundaje, Jason Ernst, Misha Bilenky, Angela Yen, Alireza Heravi-Moussavi, Pouya Kheradpour, et al. 2015. \u2018Integrative Analysis of 111 Reference Human Epigenomes\u2019. Nature 518 (7539): 317\u201330. https://doi.org/10.1038/nature14248 . Sonnet Developers. 2020. Sonnet Documentation \u2014 Sonnet Documentation (version v2.0.0). https://sonnet.readthedocs.io/en/latest/ . Tay, Yi, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2020. \u2018Long Range Arena: A Benchmark for Efficient Transformers\u2019. ArXiv:2011.04006 [Cs], November. http://arxiv.org/abs/2011.04006 . TensorFlow Developers. 2022. TensorFlow (version v2.8.2). Zenodo. https://doi.org/10.5281/ZENODO.4724125 . The ENCODE Project Consortium. 2012. \u2018An Integrated Encyclopedia of DNA Elements in the Human Genome\u2019. Nature 489 (7414): 57. https://doi.org/10.1038/nature11247 . Uffelmann, Emil, Qin Qin Huang, Nchangwi Syntia Munung, Jantina de Vries, Yukinori Okada, Alicia R. Martin, Hilary C. Martin, Tuuli Lappalainen, and Danielle Posthuma. 2021. \u2018Genome-Wide Association Studies\u2019. Nature Reviews Methods Primers 1 (1): 1\u201321. https://doi.org/10.1038/s43586-021-00056-9 . Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. \u2018Attention Is All You Need\u2019. ArXiv:1706.03762 [Cs], December. http://arxiv.org/abs/1706.03762 . Woolfe, Adam, Martin Goodson, Debbie K. Goode, Phil Snell, Gayle K. McEwen, Tanya Vavouri, Sarah F. Smith, et al. 2004. \u2018Highly Conserved Non-Coding Sequences Are Associated with Vertebrate Development\u2019. PLOS Biology 3 (1): e7. https://doi.org/10.1371/journal.pbio.0030007 . Yao, Zhuliang, Shijie Cao, Wencong Xiao, Chen Zhang, and Lanshun Nie. 2019. \u2018Balanced Sparsity for Efficient DNN Inference on GPU\u2019. Proceedings of the AAAI Conference on Artificial Intelligence 33 (July): 5676\u201383. https://doi.org/10.1609/aaai.v33i01.33015676 . Zaheer, Manzil, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, et al. 2021. \u2018Big Bird: Transformers for Longer Sequences\u2019. ArXiv:2007.14062 [Cs, Stat], January. http://arxiv.org/abs/2007.14062 . Zhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K. Wong, and Olga G. Troyanskaya. 2018. \u2018Deep Learning Sequence-Based Ab Initio Prediction of Variant Effects on Expression and Disease Risk\u2019. Nature Genetics 50 (8): 1171\u201379. https://doi.org/10.1038/s41588-018-0160-6 . Zhou, Jian, and Olga G. Troyanskaya. 2015. \u2018Predicting Effects of Noncoding Variants with Deep Learning\u2013Based Sequence Model\u2019. Nature Methods 12 (10): 931\u201334. https://doi.org/10.1038/nmeth.3547 .","title":"References"},{"location":"results/","text":"Benefits in memory usage with longer sequences To understand how sparse-attention affected the memory usage of the model, the input sequence length was increased from 131,072 bp to 786,432 bp ( Figure 1 ). To further clarify the impact of the attention method on the difference in memory usage, it was also measured when just one transformer layer was used ( Figure 1 ). The difference between the memory usage when using one transformer layer, versus the original eleven, provides an estimate of the memory required for each additional transformer layer ( Figure 2 ). Figure 1: Memory usage increases with DNA sequence length (11 vs 1 transformer layers) Memory usage scales quadratically with sequence length when the model uses self-attention. This compares to the linear scaling when sparse-attention is employed by the model. Using only one transformer layer illustrates the baseline memory usage of the model and highlights the magnitude of the difference between self- and sparse-attention. The results when using eleven transformer layers show a clear trend that the memory usage for self-attention scales quadratically compared to the linear increase for sparse-attention. With sparse-attention, the model can handle sequence lengths upwards of 786,432 bp, while the model runs out of memory (>24 GB) when using self-attention and a sequence length of 589,824 bp. The average memory usage per transformer layer provides further evidence for the difference in requirements between the two methods ( Figure 2 ). Each transformer layer requires less memory when using sparse-attention with a sequence length of 786,432 bp than when using self-attention with a sequence length of 393,216 bp. Figure 2: Average memory usage depending on sequence length At shorter sequence lengths, self-attention uses less memory per attention layer than sparse-attention, but this metric quickly reverses. At a sequence length of ~300,000 bp, the two attention mechanisms are equal. This plot can serve as a guide to help developers choose the sequence length and number of layers that are appropriate for their tasks and computation resources. Despite the benefits of using sparse-attention with longer sequences, they have a cost when using shorter sequence lengths. Until the sequence length reaches approximately 300,000 bp, self-attention uses less memory than sparse-attention. Memory usage and runtime trade-off At many of the analysed sequence lengths, training a model using sparse-attention is slower than using self-attention ( Figure 3 ). This was true whether one or eleven transformer layers were used in the model. This result is consistent with other sparse-attention models, including Performer 1 and Reformer 2 . This suggests that the method these models use to achieve linear memory dependency has a computational cost until the input is of a sufficient size. Figure 3: Slower training with sparse-attention Training the model is slower with sparse-attention at many of the tested sequence lengths. Given that a model needs to be trained for thousands of steps, using sparse-attention significantly increases the duration of training. The Long-Range Arena benchmark 3 , which compares memory-efficient Transformers against the vanilla Transformer in a range of tasks, provides further evidence of the computational speed of BigBird\u2019s sparse-attention mechanism. BigBird trained slower than the vanilla Transformer until the input sequence length was greater than 3,000. Compared to the other memory efficient Transformers, it was the second slowest (out of eight) to train 3 . Accuracy results could not be reproduced The authors released a pre-trained version of Enformer. To confirm its performance matched the results in its paper, Figure 1c 4 , we ran the model evaluation code that the authors shared (see the evaluate_model function). Adjustments were made to this code to measure the performance on the four genomic track types individually. Despite the change, the methodology remained the same. The four genomic track types are: (1) DNA accessibility, which was measured using deoxyribonuclease sequencing (DNase-Seq) and assay of transposase accessible chromatin sequencing (ATAC-Seq), (2) histone modification chromatin immunoprecipitation and sequencing (ChIP-Seq), (3) transcription factor (TF) ChIP-Seq, and (4) cap analysis of gene expression (CAGE) 5 . Each testing example has its corresponding track listed in Supplementary Table 2 of the Enformer paper. We could not reproduce the results from their paper ( Figure 4 ) and it is unclear why this is the case. The authors of the paper were contacted about this matter, but they did not provide an explanation for the difference. Going forward in this report, our results for Enformer will be used as the benchmark to provide a fair comparison against S-Enformer. Figure 4: Difference in Enformer's performance The Pearson correlation coefficients from the paper could not be reproduced. The difference between our results and Avsec et al\u2019s paper\u2019s vary considerably. The largest difference in the Pearson correlation coefficients is 0.129, for the TF ChIP-Seq tracks, while the smallest difference is 0.021, for the CAGE tracks. Achieving accuracy parity might require training the full model We attempted to achieve accuracy parity with Enformer by training S-Enformer, but only updating the weights in the sparse-attention layers. This method is similar to fine-tuning, as all of the weights were frozen except for those in the new layers. Also like fine-tuning, the benefit of this method is that it could save a significant amount of time and computational resources compared to retraining the model from scratch. Enformer was trained on 64 tensor processing units (TPU) v3 cores with a batch size of 64 (1 per core) for 150,000 steps over approximately 3 days 4 . Reducing this training process to more modest levels could help to make updating state-of-the-art models more accessible. When fine-tuning a model, it has been shown that performance parity can be achieved from just a few hours of training on a single GPU 6 . However, after training the model for 25,000 steps, with a batch size of 5 over 3 days, we were unable to achieve the same Pearson correlation coefficients as Enformer ( Figure 5 ). Figure 5: Performance parity could not be achieved 5a. Performance comparison after training 5b. Performance comparison during training The performance after training S-Enformer for 25,000 steps with a batch size of 5 compared to the pre-trained Enformer. Enformer still outperforms S-Enformer on all four of the genomic track types. S-Enformer could not achieve the same performance as Enformer after being trained for 25,000 steps with a batch size of 5. Its performance metrics were still improving at the end of the three days of training, so it is possible that parity could be achieved with further training. Since much of S-Enformer was pretrained when we began retraining, it quickly achieves non-trivial performance. Progress quickly slows, but is still continuing at the 25,000th step. It is possible that using different training parameters and further training steps could result in performance parity, but due to time limitations and parameter optimization not being the primary focus of this work, these possibilities were not explored. Highly correlated predictions To better understand the difference between Enformer and S-Enformer, the performance of their predictions were plotted together ( Figure 6 ). It is clear that the predictions are highly correlated, as the Pearson correlation coefficient for the genomic track types range from 0.863 to 0.962. Although all of the correlations are high, their range is noticeable. The results for the TF ChIP tracks are clustered tightly together, but the CAGE tracks can have significant differences. It is also noticeable that Enformer regularly, but not always, outperforms S-Enformer. S-Enformer outperforms Enformer in ~15% of testing examples on the DNase & ATAC and CAGE tracks, but only ~3% of histone modification ChIP tracks. Further investigation of this work is required to understand the cause of these discrepancies. Figure 6: The models' predictions are highly correlated S-Enformer\u2019s and Enformer\u2019s predictions are highly correlated. This is expected given the architectural and performance similarities. It is interesting that the correlations vary so much between the genomic track types and it is unclear why this is the case. Looking at a set of genomic tracks provides some insight into how Enformer and S-Enformer predict the results from experiments ( Figure 7 ). The models captured much of the variance and some of the main peaks in the tracks, but can struggle if there is significant variation over a short sequence of DNA. From calculating the mean and median gene expression for each track in the testing data, Enformer and S-Enformer both tend to overpredict compared to the experimental results. Figure 7: Much of the expression variance is captured by the models A prediction from each model on the four genomic track types alongside the experiment results. This comparison helps to illustrate the shortcomings of the models. They can capture a substantial amount of the variance in the genomic tracks and predict some of the main peaks, but struggle with large variation within a small range. Sparse attention still uses the entire receptive field The receptive field of the model was measured to confirm that no reduction was made by replacing the self-attention layers with sparse-attention. The results in Figure 8 illustrate that the model\u2019s receptive field still encompasses the entire input sequence. This means that a change in nucleotide at any base will result in a change in the predicted gene expression across an entire track. Figure 8: The receptive field is still in full use S-Enformer\u2019s and Enformer\u2019s receptive fields are both equal to the full length of the input sequence. This means that any change to the input sequence will result in a change to the predicted gene expression levels. However, how a mutation affects their predictions is different. S-Enformer is more affected by mutations that are towards the end of the input sequence, while Enformer makes larger changes to its predictions when the mutation is towards the middle of the input sequence. There is a noticeable difference in how a mutation affects the predicted expression based on attention type. When a mutation is at the start or end of the input sequence, sparse-attention predicts a greater and more uniform change in gene expression than self-attention. The change in predicted expression by self-attention is skewed towards the mutation\u2019s end of the sequence. When the mutation is towards the centre of the input sequence, both models have a visible peak where the change in predicted gene expression is most significant. A peak\u2019s location in a genomic track is correlated with the mutation\u2019s location in the input sequence. Sparse-attention differs from self-attention as its peak is greater and there is a more significant drop-off in the predicted change in gene expression as the distance increases from the peak. Choromanski, Krzysztof, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, et al. 2021. \u2018Rethinking Attention with Performers\u2019. ArXiv:2009.14794 [Cs, Stat], March. http://arxiv.org/abs/2009.14794 . \u21a9 Kitaev, Nikita, \u0141ukasz Kaiser, and Anselm Levskaya. 2020. \u2018Reformer: The Efficient Transformer\u2019. ArXiv:2001.04451 [Cs, Stat], February. http://arxiv.org/abs/2001.04451 . \u21a9 Tay, Yi, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2020. \u2018Long Range Arena: A Benchmark for Efficient Transformers\u2019. ArXiv:2011.04006 [Cs], November. http://arxiv.org/abs/2011.04006 . \u21a9 \u21a9 Avsec, \u017diga, Vikram Agarwal, Daniel Visentin, Joseph R. Ledsam, Agnieszka Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R. Kelley. 2021. \u2018Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions\u2019. Nature Methods 18 (10): 1196\u20131203. https://doi.org/10.1038/s41592-021-01252-x . \u21a9 \u21a9 Forrest et al. 2014. \u2018A Promoter-Level Mammalian Expression Atlas\u2019. Nature 507 (7493): 462. https://doi.org/10.1038/nature13182 . \u21a9 Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. \u2018BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding\u2019. ArXiv:1810.04805 [Cs], May. http://arxiv.org/abs/1810.04805 . \u21a9","title":"Results"},{"location":"results/#benefits-in-memory-usage-with-longer-sequences","text":"To understand how sparse-attention affected the memory usage of the model, the input sequence length was increased from 131,072 bp to 786,432 bp ( Figure 1 ). To further clarify the impact of the attention method on the difference in memory usage, it was also measured when just one transformer layer was used ( Figure 1 ). The difference between the memory usage when using one transformer layer, versus the original eleven, provides an estimate of the memory required for each additional transformer layer ( Figure 2 ).","title":"Benefits in memory usage with longer sequences"},{"location":"results/#figure-1-memory-usage-increases-with-dna-sequence-length-11-vs-1-transformer-layers","text":"Memory usage scales quadratically with sequence length when the model uses self-attention. This compares to the linear scaling when sparse-attention is employed by the model. Using only one transformer layer illustrates the baseline memory usage of the model and highlights the magnitude of the difference between self- and sparse-attention. The results when using eleven transformer layers show a clear trend that the memory usage for self-attention scales quadratically compared to the linear increase for sparse-attention. With sparse-attention, the model can handle sequence lengths upwards of 786,432 bp, while the model runs out of memory (>24 GB) when using self-attention and a sequence length of 589,824 bp. The average memory usage per transformer layer provides further evidence for the difference in requirements between the two methods ( Figure 2 ). Each transformer layer requires less memory when using sparse-attention with a sequence length of 786,432 bp than when using self-attention with a sequence length of 393,216 bp.","title":"Figure 1: Memory usage increases with DNA sequence length (11 vs 1 transformer layers)"},{"location":"results/#figure-2-average-memory-usage-depending-on-sequence-length","text":"At shorter sequence lengths, self-attention uses less memory per attention layer than sparse-attention, but this metric quickly reverses. At a sequence length of ~300,000 bp, the two attention mechanisms are equal. This plot can serve as a guide to help developers choose the sequence length and number of layers that are appropriate for their tasks and computation resources. Despite the benefits of using sparse-attention with longer sequences, they have a cost when using shorter sequence lengths. Until the sequence length reaches approximately 300,000 bp, self-attention uses less memory than sparse-attention.","title":"Figure 2: Average memory usage depending on sequence length"},{"location":"results/#memory-usage-and-runtime-trade-off","text":"At many of the analysed sequence lengths, training a model using sparse-attention is slower than using self-attention ( Figure 3 ). This was true whether one or eleven transformer layers were used in the model. This result is consistent with other sparse-attention models, including Performer 1 and Reformer 2 . This suggests that the method these models use to achieve linear memory dependency has a computational cost until the input is of a sufficient size.","title":"Memory usage and runtime trade-off"},{"location":"results/#figure-3-slower-training-with-sparse-attention","text":"Training the model is slower with sparse-attention at many of the tested sequence lengths. Given that a model needs to be trained for thousands of steps, using sparse-attention significantly increases the duration of training. The Long-Range Arena benchmark 3 , which compares memory-efficient Transformers against the vanilla Transformer in a range of tasks, provides further evidence of the computational speed of BigBird\u2019s sparse-attention mechanism. BigBird trained slower than the vanilla Transformer until the input sequence length was greater than 3,000. Compared to the other memory efficient Transformers, it was the second slowest (out of eight) to train 3 .","title":"Figure 3: Slower training with sparse-attention"},{"location":"results/#accuracy-results-could-not-be-reproduced","text":"The authors released a pre-trained version of Enformer. To confirm its performance matched the results in its paper, Figure 1c 4 , we ran the model evaluation code that the authors shared (see the evaluate_model function). Adjustments were made to this code to measure the performance on the four genomic track types individually. Despite the change, the methodology remained the same. The four genomic track types are: (1) DNA accessibility, which was measured using deoxyribonuclease sequencing (DNase-Seq) and assay of transposase accessible chromatin sequencing (ATAC-Seq), (2) histone modification chromatin immunoprecipitation and sequencing (ChIP-Seq), (3) transcription factor (TF) ChIP-Seq, and (4) cap analysis of gene expression (CAGE) 5 . Each testing example has its corresponding track listed in Supplementary Table 2 of the Enformer paper. We could not reproduce the results from their paper ( Figure 4 ) and it is unclear why this is the case. The authors of the paper were contacted about this matter, but they did not provide an explanation for the difference. Going forward in this report, our results for Enformer will be used as the benchmark to provide a fair comparison against S-Enformer.","title":"Accuracy results could not be reproduced"},{"location":"results/#figure-4-difference-in-enformers-performance","text":"The Pearson correlation coefficients from the paper could not be reproduced. The difference between our results and Avsec et al\u2019s paper\u2019s vary considerably. The largest difference in the Pearson correlation coefficients is 0.129, for the TF ChIP-Seq tracks, while the smallest difference is 0.021, for the CAGE tracks.","title":"Figure 4: Difference in Enformer's performance"},{"location":"results/#achieving-accuracy-parity-might-require-training-the-full-model","text":"We attempted to achieve accuracy parity with Enformer by training S-Enformer, but only updating the weights in the sparse-attention layers. This method is similar to fine-tuning, as all of the weights were frozen except for those in the new layers. Also like fine-tuning, the benefit of this method is that it could save a significant amount of time and computational resources compared to retraining the model from scratch. Enformer was trained on 64 tensor processing units (TPU) v3 cores with a batch size of 64 (1 per core) for 150,000 steps over approximately 3 days 4 . Reducing this training process to more modest levels could help to make updating state-of-the-art models more accessible. When fine-tuning a model, it has been shown that performance parity can be achieved from just a few hours of training on a single GPU 6 . However, after training the model for 25,000 steps, with a batch size of 5 over 3 days, we were unable to achieve the same Pearson correlation coefficients as Enformer ( Figure 5 ).","title":"Achieving accuracy parity might require training the full model"},{"location":"results/#figure-5-performance-parity-could-not-be-achieved","text":"5a. Performance comparison after training 5b. Performance comparison during training The performance after training S-Enformer for 25,000 steps with a batch size of 5 compared to the pre-trained Enformer. Enformer still outperforms S-Enformer on all four of the genomic track types. S-Enformer could not achieve the same performance as Enformer after being trained for 25,000 steps with a batch size of 5. Its performance metrics were still improving at the end of the three days of training, so it is possible that parity could be achieved with further training. Since much of S-Enformer was pretrained when we began retraining, it quickly achieves non-trivial performance. Progress quickly slows, but is still continuing at the 25,000th step. It is possible that using different training parameters and further training steps could result in performance parity, but due to time limitations and parameter optimization not being the primary focus of this work, these possibilities were not explored.","title":"Figure 5: Performance parity could not be achieved"},{"location":"results/#highly-correlated-predictions","text":"To better understand the difference between Enformer and S-Enformer, the performance of their predictions were plotted together ( Figure 6 ). It is clear that the predictions are highly correlated, as the Pearson correlation coefficient for the genomic track types range from 0.863 to 0.962. Although all of the correlations are high, their range is noticeable. The results for the TF ChIP tracks are clustered tightly together, but the CAGE tracks can have significant differences. It is also noticeable that Enformer regularly, but not always, outperforms S-Enformer. S-Enformer outperforms Enformer in ~15% of testing examples on the DNase & ATAC and CAGE tracks, but only ~3% of histone modification ChIP tracks. Further investigation of this work is required to understand the cause of these discrepancies.","title":"Highly correlated predictions"},{"location":"results/#figure-6-the-models-predictions-are-highly-correlated","text":"S-Enformer\u2019s and Enformer\u2019s predictions are highly correlated. This is expected given the architectural and performance similarities. It is interesting that the correlations vary so much between the genomic track types and it is unclear why this is the case. Looking at a set of genomic tracks provides some insight into how Enformer and S-Enformer predict the results from experiments ( Figure 7 ). The models captured much of the variance and some of the main peaks in the tracks, but can struggle if there is significant variation over a short sequence of DNA. From calculating the mean and median gene expression for each track in the testing data, Enformer and S-Enformer both tend to overpredict compared to the experimental results.","title":"Figure 6: The models' predictions are highly correlated"},{"location":"results/#figure-7-much-of-the-expression-variance-is-captured-by-the-models","text":"A prediction from each model on the four genomic track types alongside the experiment results. This comparison helps to illustrate the shortcomings of the models. They can capture a substantial amount of the variance in the genomic tracks and predict some of the main peaks, but struggle with large variation within a small range.","title":"Figure 7: Much of the expression variance is captured by the models"},{"location":"results/#sparse-attention-still-uses-the-entire-receptive-field","text":"The receptive field of the model was measured to confirm that no reduction was made by replacing the self-attention layers with sparse-attention. The results in Figure 8 illustrate that the model\u2019s receptive field still encompasses the entire input sequence. This means that a change in nucleotide at any base will result in a change in the predicted gene expression across an entire track.","title":"Sparse attention still uses the entire receptive field"},{"location":"results/#figure-8-the-receptive-field-is-still-in-full-use","text":"S-Enformer\u2019s and Enformer\u2019s receptive fields are both equal to the full length of the input sequence. This means that any change to the input sequence will result in a change to the predicted gene expression levels. However, how a mutation affects their predictions is different. S-Enformer is more affected by mutations that are towards the end of the input sequence, while Enformer makes larger changes to its predictions when the mutation is towards the middle of the input sequence. There is a noticeable difference in how a mutation affects the predicted expression based on attention type. When a mutation is at the start or end of the input sequence, sparse-attention predicts a greater and more uniform change in gene expression than self-attention. The change in predicted expression by self-attention is skewed towards the mutation\u2019s end of the sequence. When the mutation is towards the centre of the input sequence, both models have a visible peak where the change in predicted gene expression is most significant. A peak\u2019s location in a genomic track is correlated with the mutation\u2019s location in the input sequence. Sparse-attention differs from self-attention as its peak is greater and there is a more significant drop-off in the predicted change in gene expression as the distance increases from the peak. Choromanski, Krzysztof, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, et al. 2021. \u2018Rethinking Attention with Performers\u2019. ArXiv:2009.14794 [Cs, Stat], March. http://arxiv.org/abs/2009.14794 . \u21a9 Kitaev, Nikita, \u0141ukasz Kaiser, and Anselm Levskaya. 2020. \u2018Reformer: The Efficient Transformer\u2019. ArXiv:2001.04451 [Cs, Stat], February. http://arxiv.org/abs/2001.04451 . \u21a9 Tay, Yi, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2020. \u2018Long Range Arena: A Benchmark for Efficient Transformers\u2019. ArXiv:2011.04006 [Cs], November. http://arxiv.org/abs/2011.04006 . \u21a9 \u21a9 Avsec, \u017diga, Vikram Agarwal, Daniel Visentin, Joseph R. Ledsam, Agnieszka Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R. Kelley. 2021. \u2018Effective Gene Expression Prediction from Sequence by Integrating Long-Range Interactions\u2019. Nature Methods 18 (10): 1196\u20131203. https://doi.org/10.1038/s41592-021-01252-x . \u21a9 \u21a9 Forrest et al. 2014. \u2018A Promoter-Level Mammalian Expression Atlas\u2019. Nature 507 (7493): 462. https://doi.org/10.1038/nature13182 . \u21a9 Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. \u2018BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding\u2019. ArXiv:1810.04805 [Cs], May. http://arxiv.org/abs/1810.04805 . \u21a9","title":"Figure 8: The receptive field is still in full use"}]}