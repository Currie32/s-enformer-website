Deep learning models have made significant gains in predicting gene expression from a sequence of DNA. A key limitation of these models is that they still lack the ability to use long enough sequences to include all regulatory elements in their predictions. By replacing the self-attention from the Enformer model with BigBird’s sparse-attention, we have taken one step closer to overcoming this obstacle. Although we lacked the time and computational resources to train the model from scratch and demonstrate that using sparse-attention and a longer input sequence will achieve more accurate results, we are confident that this will be the case. This confidence is based on the state-of-the-art results achieved by BigBird (Zaheer et al. 2021; Tay et al. 2020) from its novel architecture and earlier models scoring higher accuracy through the use of longer input sequences (Avsec, Agarwal, et al. 2021; Kelley et al. 2018; Zhou et al. 2018).

## Limitations to model training

A possible explanation for not achieving accuracy parity with Enformer is the constraint placed on the model during training. When a model is being trained from scratch, all of its weights update in order to reduce the loss. This allows the model to make the best use of its architecture. When training S-Enformer, the model could not update in a cohesive way as all of the weights were frozen except for those in the sparse-attention layers. The sparse-attention layers’ weights were forced to update in a constrained environment and possibly not make optimal use of their design. Given that the self-attention layers have a different architecture to the sparse-attention layers, it might not be possible for the model to achieve its optimal performance when only the sparse-attention layer weights can update.

## Increase prediction resolution

Enformer was designed to predict human and mouse genomic tracks at 128-bp resolution. Depending on the task, it could be beneficial to have a more precise resolution, such as to a single base. BPNet is one such model that was designed for nucleotide-resolution prediction (Avsec, Weilert, et al. 2021). Its input sequence length is 2,068-bp, which is longer than if Enformer was scaled down to the same resolution (1,538 = 196,608 / 128). However, Enformer should be able to use a longer input sequence than 1,538-bp since shorter sequences use less memory. Nonetheless, using S-Enformer’s architecture should allow for a much longer input sequence. This is because BPNet (Avsec, Weilert, et al. 2021) uses dilated convolutional layers, which were also used by Basenji (Kelley et al. 2018), the prior work to Enformer. Enformer has already demonstrated that using transformers instead of dilated convolutional layers allows for a longer input sequence length (Avsec, Agarwal, et al. 2021). The addition of sparse-attention should further these gains.

A nucleotide-resolution model could be applied to GWASs to help identify the causal variant(s) for a disease or trait. This is due to the unidirectional flow of information in a model. For example, the expression level of a DNA sequence is predicted by a model, then a single mutation is made to the DNA sequence, once again, the expression is predicted. The difference in expression is solely the result of the mutation. This high resolution measurement by a model can complement the wide search space of a GWAS to better identify nucleotides of interest.

## More transformer layers may be required

In a self-attention layer, all nodes are connected with each other. This allows information to easily be shared across the neural network. In BigBird’s sparse-attention, not all nodes are connected within a layer. For two unconnected nodes in a layer to share information, they must pass information via their connections with nodes in other layers. Given the large input sequence length, more layers than the currently used eleven could be required to achieve accuracy parity with Enformer. As a default, Bigbird uses sixteen transformer layers, which suggests that more layers might be required for optimal performance (Zaheer et al. 2021). Depending on the number of additional transformer layers required to match Enformer’s performance, the benefits of sparse attention could deteriorate.

## Parameter optimisation could improve results

Optimising S-Enformer’s training parameters and architecture were not a primary focus of this work. Therefore, there are potential gains to be made on our results.

The learning rate was held constant at 0.0001 during training. Alternative rates, or starting with a higher rate and decaying it during training could result in faster training and improved performance.

The default model architecture values were used to train S-Enformer. With respect to the parameters for the sparse-attention layers, values that could be changed include the block size, number of random blocks, number of sliding blocks, and number of global blocks. Each of these values will affect the amount of attention used by the model. Given the complexities of this task, it is possible that the model is not attending to enough of the data and increasing some of these values could be beneficial.

## Alternative sparse-attention methods

There are multiple other sparse-attention methods that are worth exploring (Tay et al. 2020). Two notable ones are the Linear Transformer (Katharopoulos et al. 2020) and Performer (Choromanski et al. 2021). Both of these models train faster and have a lower peak memory usage than BigBird (Tay et al. 2020). When looking at the average accuracy of predictions, Bigbird is the best performing model, but the more relevant metric is likely the text classification results (Tay et al. 2020) as this is the most similar task to predicting gene expression. In the text classification task, both the Linear Transformer and Performer outperform Bigbird, which suggests that they could achieve better results in predicting gene expression (Tay et al. 2020).
