Genome wide association studies (GWAS) have identified thousands of potential genetic variants that are associated with human diseases and traits[^1]. The vast majority of these variants are located in non-coding regions, suggesting that gene regulation has a significant role in the health of an individual (Edwards et al. 2013; Leslie, O’Donnell, and Johnson 2014). Adverse changes to gene expression have been shown to lead to an increased risk of disease (Edwards et al. 2013; Albert and Kruglyak 2015). It is therefore critical to identify disease-causing genetic mutations to better understand and help develop suitable treatments. However, due to the limitations of GWAS, these studies are incapable of regularly finding the causal variant as they are  limited to analysing only common variants and linkage disequilibrium makes it challenging to differ between association and causality (Avsec, Agarwal, et al. 2021).

The shortcomings of GWAS can be alleviated by advances in deep learning models (Zhou and Troyanskaya 2015; Kelley, Snoek, and Rinn 2016; Kelley et al. 2018; Zhou et al. 2018; Pei et al. 2021; Avsec, Agarwal, et al. 2021). These models predict gene expression using only a sequence of DNA to help understand the impact of non-coding variants. The early work of DeepSEA demonstrated the feasibility of using deep convolutional neural networks (CNN) to predict gene expression from a sequence of DNA (Zhou and Troyanskaya 2015). This outperformed the previous state-of-the-art method, a support vector machine, as deep CNNs can better aggregate the non-linear interactions across a DNA sequence that result in the gene expression level (Mamoshina et al. 2016).

Further advancements in deep learning models were made by Basset (Kelley, Snoek, and Rinn 2016), Basenji (Kelley et al. 2018), ExPecto (Zhou et al. 2018), DeepFun (Pei et al. 2021), and Enformer (Avsec, Agarwal, et al. 2021). These models used data from an increasing number of cell and tissue types, a longer input sequence of DNA, and more advanced model architectures to achieve better results.

It is necessary for a model to be trained using a variety of cell and tissue types, as transcription regulation can differ between them (Hobert 2008). In addition, disease-causing variants tend to alter gene regulation within specific cell and tissues types (Aguet et al. 2017). Therefore, it is critical that a model can generalise to a wide range of cell and tissue types to maximise its utility.  Fortunately, many organisations have annotated epigenetic and expression profiles in a wide range of cell types (The ENCODE Project Consortium 2012; Forrest et al. 2014; Roadmap Epigenomics Consortium et al. 2015). This work has provided recent deep learning models with their training, validation, and testing data. The addition of mouse genomic data was a significant advancement as it provided training examples from tissues that are unable to be collected easily from humans (Kelley 2020). The author demonstrated that a model trained on both human and mouse data outperformed a model trained solely on human data, thus demonstrating that the conserved nature of non-coding DNA between organisms can be used to improve a model’s accuracy (Woolfe et al. 2004; Pennacchio et al. 2006).

The length of the input DNA sequence used by deep learning models has increased significantly from 1kb for DeepSEA (Zhou and Troyanskaya 2015) to almost 200kb in Enformer (Avsec, Agarwal, et al. 2021). Increasing the amount of DNA used by a model is essential as some regulatory elements, such as enhancers, influence the expression of a gene from hundreds of thousands of base pairs away (Levine 2010; Long, Prescott, and Wysocka 2016). Some research estimates that 16% of enhancer-gene pairs could be more than 100kb away (Avsec, Agarwal, et al. 2021). To achieve even better results, new models will require longer DNA sequences to capture the remaining regulatory elements that are out of scope of the current models.

The architecture of deep learning models has evolved in recent years. Advancements have been made by using more convolutional layers to learn more abstract features (Zhou et al. 2018), densely connected dilated convolutions to share information across longer distances (Kelley et al. 2018), and transformers with self-attention to increase the receptive field of the model and more accurately capture distal information (Avsec, Agarwal, et al. 2021). A key limitation to the accuracy gains made by Enformer is its memory usage, which scales quadratically with sequence length due to its self-attention mechanism.

To overcome this limitation, we propose a new model, Sparse-Enformer (S-Enformer) that replaces the self-attention with sparse-attention from the BigBird model (Zaheer et al. 2021). Using sparse-attention reduces the quadratic increase in memory usage to linear. This allows for a significantly longer DNA sequence to be used as input to a model, which should result in further accuracy improvements. We also demonstrate that S-Enformer can be trained and evaluated in exactly the same method as Enformer, which simplifies any changes to using this new model.

[^1]: Uffelmann, Emil, Qin Qin Huang, Nchangwi Syntia Munung, Jantina de Vries, Yukinori Okada, Alicia R. Martin, Hilary C. Martin, Tuuli Lappalainen, and Danielle Posthuma. 2021. ‘Genome-Wide Association Studies’. Nature Reviews Methods Primers 1 (1): 1–21. <a href= https://doi.org/10.1038/s43586-021-00056-9>https://doi.org/10.1038/s43586-021-00056-9</a>.
